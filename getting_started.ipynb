{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "---\n",
    "\n",
    "## Web scraping and analysis\n",
    "\n",
    "This Jupyter notebook includes some code to get you started with web scraping. We will use a package called `BeautifulSoup` to collect the data from the web. Once you've collected your data and saved it into a local `.csv` file you should start with your analysis.\n",
    "\n",
    "### Scraping data from Skytrax\n",
    "\n",
    "If you visit [https://www.airlinequality.com] you can see that there is a lot of data there. For this task, we are only interested in reviews related to British Airways and the Airline itself.\n",
    "\n",
    "If you navigate to this link: [https://www.airlinequality.com/airline-reviews/british-airways] you will see this data. Now, we can use `Python` and `BeautifulSoup` to collect all the links to the reviews and then to collect the text data on each of the individual review links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping page 1\n",
      "   ---> 10 total reviews\n",
      "Scraping page 2\n",
      "   ---> 20 total reviews\n",
      "Scraping page 3\n",
      "   ---> 30 total reviews\n",
      "Scraping page 4\n",
      "   ---> 40 total reviews\n",
      "Scraping page 5\n"
     ]
    }
   ],
   "source": [
    "base_url = \"https://www.airlinequality.com/airline-reviews/british-airways\"\n",
    "pages = 177\n",
    "page_size = 10\n",
    "\n",
    "reviews = []\n",
    "\n",
    "#declare dictionary\n",
    "data_dict = {}\n",
    "# for i in range(1, pages + 1):\n",
    "review_count = 1\n",
    "for i in range(1, pages + 1):\n",
    "\n",
    "    print(f\"Scraping page {i}\")\n",
    "\n",
    "    # Create URL to collect links from paginated data\n",
    "    url = f\"{base_url}/page/{i}/?sortby=post_date%3ADesc&pagesize={page_size}\"\n",
    "\n",
    "    # Collect HTML data from this page\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Parse content\n",
    "    content = response.content\n",
    "    parsed_content = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "    for para in parsed_content.find_all(\"article\",{\"itemprop\":\"review\"}):\n",
    "        new_dict = {}\n",
    "        review_text = para.find_all(\"div\", {\"class\": \"text_content\"})\n",
    "        new_dict['reviews'] = review_text[0].get_text() # store reviews in dictionary\n",
    "\n",
    "        table = para.select('table.review-ratings')\n",
    "        length_rating = len(table[0].find_all(\"td\",{\"class\":\"review-rating-header\"}))\n",
    "        ratings = []\n",
    "        count = 0\n",
    "        stars_label_names = []\n",
    "        for j in range(0,length_rating*2,2):\n",
    "            type_rate = table[0].find_all(\"td\")[j].attrs['class'][1]\n",
    "            columns = table[0].find_all(\"td\")[j].findNext('td').get_text()\n",
    "            \n",
    "            if(columns == '12345'):\n",
    "                stars_label = table[0].find_all(\"td\")[j].get_text()\n",
    "                stars_label_names.append(stars_label)\n",
    "                count+=1\n",
    "            else:\n",
    "                new_dict[type_rate] = columns\n",
    "    \n",
    "        stars_count = 0\n",
    "        stars_index = 0\n",
    "        \n",
    "        for k in range(0,count*5,1):\n",
    "            stars = table[0].find_all(\"span\")[k].attrs['class']\n",
    "            if(len(stars) == 2):\n",
    "                stars_count+=1\n",
    "            if((k+1)%5 == 0):\n",
    "                #assign stars count\n",
    "                new_dict[stars_label_names[stars_index]] = stars_count\n",
    "                stars_index+=1\n",
    "                stars_count = 0\n",
    "        data_dict[review_count] = new_dict\n",
    "        review_count+=1\n",
    "\n",
    "    print(f\"   ---> {len(data_dict)} total reviews\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data_dict)\n",
    "df = df.transpose()\n",
    "df.shape\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               reviews\n",
      "0    Not Verified |  They changed our Flights from ...\n",
      "1    Not Verified |  At Copenhagen the most chaotic...\n",
      "2    ✅ Trip Verified |  Worst experience of my life...\n",
      "3    ✅ Trip Verified |  Due to code sharing with Ca...\n",
      "4    ✅ Trip Verified |  LHR check in was quick at t...\n",
      "..                                                 ...\n",
      "995  ✅ Trip Verified |  Linate to London. The morni...\n",
      "996  ✅ Trip Verified | Flew British Airways from JK...\n",
      "997  ✅ Trip Verified | I have flown British Airways...\n",
      "998  ✅ Trip Verified | We can not fault the new 'Cl...\n",
      "999  ✅ Trip Verified |  Very disappointing experien...\n",
      "\n",
      "[1000 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['reviews'] = df['reviews'].apply(lambda x:x.split(\"|\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2476954167.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\DEGGIE\\AppData\\Local\\Temp\\ipykernel_16128\\2476954167.py\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    df[df['reviews']\u001b[0m\n\u001b[1;37m                    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "df[df['reviews']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling,\n",
    "## Sentiment Analysis\n",
    "## wordclouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import packages\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DEGGIE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DEGGIE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DEGGIE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt') \n",
    "nltk.download('wordnet')   \n",
    "from nltk import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer   \n",
    "class LemmaTokenizer: \n",
    "    def __init__(self): \n",
    "        self.wnl = WordNetLemmatizer() \n",
    "    def __call__(self, doc): \n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if (t.isalpha() and len(t) >= 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DEGGIE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_stopwords = ['could', 'doe', 'ha', 'might', 'must', 'need', 'sha', 'wa', 'wo', 'would']\n",
    "stp_word = nltk.corpus.stopwords.words('english')\n",
    "stp_word.extend(new_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer(stop_words=stopwords.words('english'), tokenizer=LemmaTokenizer(), lowercase=True, max_df=0.5, min_df=10)\n",
    "x_counts = count_vect.fit_transform(df['reviews'].to_numpy())\n",
    "# x_counts.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['able', 'absolutely', 'accept', ..., 'york', 'young', 'zero'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "x_tfidf = tfidf_transformer.fit_transform(x_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88171134, 0.05048518, 0.06780348],\n",
       "       [0.68497998, 0.08726333, 0.22775669],\n",
       "       [0.85626632, 0.04579329, 0.09794038],\n",
       "       ...,\n",
       "       [0.37657081, 0.04012432, 0.58330487],\n",
       "       [0.08426215, 0.04413684, 0.87160101],\n",
       "       [0.05395486, 0.04928959, 0.89675555]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimension = 3\n",
    "lda = LDA(n_components=dimension, random_state=42)\n",
    "lda_array = lda.fit_transform(x_tfidf)\n",
    "lda_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['customer', 'hour', 'airway', 'british', 'airline'], ['thank', 'clothes', 'sydney', 'suitcase', 'wish'], ['seat', 'crew', 'good', 'food', 'cabin']]\n"
     ]
    }
   ],
   "source": [
    "components = [lda.components_[i] for i in range(len(lda.components_))]\n",
    "features = count_vect.get_feature_names_out()\n",
    "important_words = [sorted(features, key = lambda x: components[j][np.where(features == x )], reverse = True)[:5] for j in range(len(components))]\n",
    "print(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.reviews.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data/BA_reviews.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! Now you have your dataset for this task! The loops above collected 1000 reviews by iterating through the paginated pages on the website. However, if you want to collect more data, try increasing the number of pages!\n",
    "\n",
    " The next thing that you should do is clean this data to remove any unnecessary text from each of the rows. For example, \"✅ Trip Verified\" can be removed from each row if it exists, as it's not relevant to what we want to investigate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "4f7924c4c56b083e0e50eadfe7ef592a7a8ef70df33a0047f82280e6be1afe15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
